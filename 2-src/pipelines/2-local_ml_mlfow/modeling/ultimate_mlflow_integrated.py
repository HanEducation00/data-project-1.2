#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTIMATE + MLFLOW: Enterprise Model Registry & Tracking
6.3M Records + R¬≤=0.9873 + MLflow Integration
"""

import sys
import os
import math
import time
import json
from datetime import datetime, timedelta

# MLflow imports
import mlflow
import mlflow.spark
from mlflow.models.signature import infer_signature
from mlflow.types.schema import Schema, ColSpec

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data_processing'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utils'))

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum as spark_sum, avg as spark_avg, max as spark_max, min as spark_min,
    date_format, dayofweek, month, quarter, hour, minute,
    when, lit, sin, cos, count, lag, rand
)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline

def setup_mlflow_tracking():
    """
    MLflow tracking setup for enterprise model registry
    """
    print("üìä SETTING UP MLFLOW TRACKING...")
    
    # Set tracking URI (local for now)
    tracking_uri = "http://mlflow-server:5000"
    mlflow.set_tracking_uri(tracking_uri)
    
    # Set experiment
    experiment_name = "Ultimate_6M_Energy_Forecasting"
    try:
        experiment_id = mlflow.create_experiment(experiment_name)
        print(f"‚úÖ Created new experiment: {experiment_name}")
    except:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        experiment_id = experiment.experiment_id
        print(f"‚úÖ Using existing experiment: {experiment_name}")
    
    mlflow.set_experiment(experiment_name)
    
    print(f"üìÅ MLflow tracking URI: {tracking_uri}")
    print(f"üß™ Experiment: {experiment_name}")
    print(f"üÜî Experiment ID: {experiment_id}")
    
    return tracking_uri, experiment_name, experiment_id

def ultimate_mlflow_pipeline():
    """
    Ultimate pipeline with full MLflow integration
    """
    print("üöÄ ULTIMATE + MLFLOW ENTERPRISE PIPELINE!")
    print("üìä 6.3M Records + MLflow Tracking + Model Registry")
    print("="*80)
    
    # Setup MLflow
    tracking_uri, experiment_name, experiment_id = setup_mlflow_tracking()
    
    # Start MLflow run
    with mlflow.start_run(run_name=f"Ultimate_6M_Records_{datetime.now().strftime('%Y%m%d_%H%M%S')}") as run:
        
        print(f"üÜî MLflow Run ID: {run.info.run_id}")
        
        overall_start = time.time()
        
        # Log initial parameters
        mlflow.log_param("dataset_size_target", "6,340,608")
        mlflow.log_param("pipeline_type", "Enterprise_Full_Year")
        mlflow.log_param("feature_engineering", "Advanced_Lag_Features")
        mlflow.log_param("model_type", "GBTRegressor")
        
        try:
            # 1. DATA EXPORT (reuse previous successful export)
            print("\n1Ô∏è‚É£ REUSING SUCCESSFUL EXPORT...")
            export_dir = "/tmp/ultimate_full_year"
            
            if not os.path.exists(export_dir):
                print("‚ùå Export directory not found! Running export first...")
                # Run export (previous code)
                from ultimate_full_year_ml import export_full_year_postgresql_data, create_ultimate_export_directory
                export_dir = create_ultimate_export_directory()
                exported_files, total_exported = export_full_year_postgresql_data(export_dir)
            else:
                print(f"‚úÖ Using existing export: {export_dir}")
                total_exported = 6340608  # Known value
            
            mlflow.log_param("data_source", "PostgreSQL")
            mlflow.log_param("export_directory", export_dir)
            mlflow.log_metric("exported_records", total_exported)
            
            # 2. SPARK SESSION
            print("\n2Ô∏è‚É£ ENTERPRISE SPARK SESSION...")
            spark = SparkSession.builder \
                .appName("Ultimate_MLflow_Integration") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .getOrCreate()
            
            mlflow.log_param("spark_version", spark.version)
            mlflow.log_param("spark_adaptive_enabled", "true")
            
            # 3. DATA LOADING
            print("\n3Ô∏è‚É£ DATA LOADING...")
            loading_start = time.time()
            
            csv_pattern = f"{export_dir}/*.csv*"
            df = spark.read \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .option("compression", "gzip") \
                .csv(csv_pattern)
            
            df = df.withColumn("full_timestamp", col("full_timestamp").cast(TimestampType()))
            df.persist()
            
            record_count = df.count()
            loading_time = time.time() - loading_start
            
            mlflow.log_metric("loaded_records", record_count)
            mlflow.log_metric("loading_time_seconds", loading_time)
            
            print(f"‚úÖ Loaded {record_count:,} records in {loading_time:.1f}s")
            
            # 4. FEATURE ENGINEERING
            print("\n4Ô∏è‚É£ FEATURE ENGINEERING...")
            feature_start = time.time()
            
            # Basic temporal features
            df = df.withColumn("hour", hour("full_timestamp")) \
                  .withColumn("month", month("full_timestamp")) \
                  .withColumn("dayofweek", dayofweek("full_timestamp")) \
                  .withColumn("quarter", quarter("full_timestamp")) \
                  .withColumn("is_weekend", when(col("dayofweek").isin([1, 7]), 1).otherwise(0)) \
                  .withColumn("season", 
                    when(col("month").isin([12, 1, 2]), 0)
                    .when(col("month").isin([3, 4, 5]), 1)
                    .when(col("month").isin([6, 7, 8]), 2)
                    .otherwise(3))
            
            # Cyclical features
            PI = lit(math.pi)
            df = df.withColumn("sin_hour", sin(col("hour") * (2.0 * PI / 24))) \
                  .withColumn("cos_hour", cos(col("hour") * (2.0 * PI / 24))) \
                  .withColumn("sin_month", sin(col("month") * (2.0 * PI / 12))) \
                  .withColumn("cos_month", cos(col("month") * (2.0 * PI / 12)))
            
            # Advanced lag features (the money-makers!)
            window_spec = Window.partitionBy("customer_id").orderBy("full_timestamp")
            
            df = df.withColumn("load_lag_1h", lag("load_percentage", 4).over(window_spec)) \
                  .withColumn("load_lag_6h", lag("load_percentage", 24).over(window_spec)) \
                  .withColumn("load_lag_24h", lag("load_percentage", 96).over(window_spec)) \
                  .withColumn("rolling_avg_4h", spark_avg("load_percentage").over(window_spec.rowsBetween(-16, 0))) \
                  .withColumn("rolling_avg_24h", spark_avg("load_percentage").over(window_spec.rowsBetween(-96, 0))) \
                  .withColumn("rolling_max_24h", spark_max("load_percentage").over(window_spec.rowsBetween(-96, 0)))
            
            # Fill nulls
            df = df.na.fill({
                "load_lag_1h": 0, "load_lag_6h": 0, "load_lag_24h": 0,
                "rolling_avg_4h": 0, "rolling_avg_24h": 0, "rolling_max_24h": 0
            })
            
            feature_time = time.time() - feature_start
            mlflow.log_metric("feature_engineering_time_seconds", feature_time)
            
            # Log feature engineering parameters
            mlflow.log_param("lag_features", "1h,6h,24h")
            mlflow.log_param("rolling_windows", "4h,24h")
            mlflow.log_param("cyclical_features", "hour,month")
            
            print(f"‚úÖ Feature engineering completed in {feature_time:.1f}s")
            
            # 5. DAILY AGGREGATION
            print("\n5Ô∏è‚É£ DAILY AGGREGATION...")
            agg_start = time.time()
            
            daily_df = df.groupBy(
                date_format("full_timestamp", "yyyy-MM-dd").alias("date"),
                "customer_id"
            ).agg(
                spark_sum("load_percentage").alias("daily_energy"),
                spark_avg("load_percentage").alias("daily_avg"),
                spark_max("load_percentage").alias("daily_peak"),
                spark_min("load_percentage").alias("daily_min"),
                spark_avg("sin_hour").alias("avg_sin_hour"),
                spark_avg("cos_hour").alias("avg_cos_hour"),
                spark_avg("sin_month").alias("avg_sin_month"),
                spark_avg("cos_month").alias("avg_cos_month"),
                spark_avg("season").alias("season"),
                spark_avg("is_weekend").alias("is_weekend"),
                spark_avg("load_lag_1h").alias("avg_lag_1h"),
                spark_avg("load_lag_6h").alias("avg_lag_6h"),
                spark_avg("load_lag_24h").alias("avg_lag_24h"),
                spark_avg("rolling_avg_4h").alias("avg_rolling_4h"),
                spark_avg("rolling_avg_24h").alias("avg_rolling_24h"),
                count("*").alias("hourly_count")
            )
            
            daily_df = daily_df.withColumn("date", col("date").cast("timestamp")) \
                              .withColumn("month", month("date")) \
                              .withColumn("dayofweek", dayofweek("date")) \
                              .withColumn("quarter", quarter("date"))
            
            daily_count = daily_df.count()
            agg_time = time.time() - agg_start
            
            mlflow.log_metric("daily_records", daily_count)
            mlflow.log_metric("aggregation_time_seconds", agg_time)
            
            print(f"‚úÖ Daily aggregation: {daily_count:,} records in {agg_time:.1f}s")
            
            # 6. TRAIN/VALIDATION/TEST SPLITS
            print("\n6Ô∏è‚É£ DATA SPLITS...")
            daily_df = daily_df.withColumn("random", rand(seed=42))
            
            train_df = daily_df.filter(col("random") < 0.70)
            validation_df = daily_df.filter((col("random") >= 0.70) & (col("random") < 0.85))
            test_df = daily_df.filter(col("random") >= 0.85)
            
            train_count = train_df.count()
            validation_count = validation_df.count()
            test_count = test_df.count()
            
            # Log split information
            mlflow.log_param("train_split", 0.70)
            mlflow.log_param("validation_split", 0.15)
            mlflow.log_param("test_split", 0.15)
            mlflow.log_metric("train_records", train_count)
            mlflow.log_metric("validation_records", validation_count)
            mlflow.log_metric("test_records", test_count)
            
            print(f"‚úÖ Splits - Train: {train_count:,}, Val: {validation_count:,}, Test: {test_count:,}")
            
            # Remove random column and cache
            train_df = train_df.drop("random").cache()
            validation_df = validation_df.drop("random").cache()
            test_df = test_df.drop("random").cache()
            
            # 7. ML PIPELINE SETUP
            print("\n7Ô∏è‚É£ ML PIPELINE...")
            
            features = [
                "month", "dayofweek", "quarter", "season", "is_weekend",
                "avg_sin_hour", "avg_cos_hour", "avg_sin_month", "avg_cos_month",
                "avg_lag_1h", "avg_lag_6h", "avg_lag_24h",
                "avg_rolling_4h", "avg_rolling_24h",
                "hourly_count", "daily_min", "daily_peak"
            ]
            target = "daily_energy"
            
            # Log feature information
            mlflow.log_param("features", features)
            mlflow.log_param("target", target)
            mlflow.log_param("num_features", len(features))
            
            # GBT hyperparameters
            max_iter = 50
            max_depth = 10
            step_size = 0.05
            subsampling_rate = 0.8
            
            # Log hyperparameters
            mlflow.log_param("gbt_max_iter", max_iter)
            mlflow.log_param("gbt_max_depth", max_depth)
            mlflow.log_param("gbt_step_size", step_size)
            mlflow.log_param("gbt_subsampling_rate", subsampling_rate)
            mlflow.log_param("gbt_feature_subset_strategy", "sqrt")
            
            # Create pipeline
            assembler = VectorAssembler(inputCols=features, outputCol="features")
            scaler = StandardScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=True)
            gbt = GBTRegressor(
                featuresCol="scaled_features",
                labelCol=target,
                predictionCol="prediction",
                maxIter=max_iter,
                maxDepth=max_depth,
                stepSize=step_size,
                subsamplingRate=subsampling_rate,
                featureSubsetStrategy="sqrt"
            )
            
            pipeline = Pipeline(stages=[assembler, scaler, gbt])
            
            # 8. MODEL TRAINING
            print("\n8Ô∏è‚É£ MODEL TRAINING...")
            training_start = time.time()
            
            model = pipeline.fit(train_df)
            training_time = time.time() - training_start
            
            mlflow.log_metric("training_time_seconds", training_time)
            
            print(f"‚úÖ Model training completed in {training_time:.1f}s")
            
            # 9. MODEL EVALUATION
            print("\n9Ô∏è‚É£ MODEL EVALUATION...")
            
            evaluator = RegressionEvaluator(labelCol=target, predictionCol="prediction")
            
            # Train metrics
            train_predictions = model.transform(train_df)
            train_rmse = evaluator.evaluate(train_predictions, {evaluator.metricName: "rmse"})
            evaluator.setMetricName("r2")
            train_r2 = evaluator.evaluate(train_predictions)
            
            # Validation metrics
            validation_predictions = model.transform(validation_df)
            evaluator.setMetricName("rmse")
            val_rmse = evaluator.evaluate(validation_predictions)
            evaluator.setMetricName("r2")
            val_r2 = evaluator.evaluate(validation_predictions)
            
            # Test metrics
            test_predictions = model.transform(test_df)
            evaluator.setMetricName("rmse")
            test_rmse = evaluator.evaluate(test_predictions)
            evaluator.setMetricName("r2")
            test_r2 = evaluator.evaluate(test_predictions)
            
            # Log all metrics
            mlflow.log_metric("train_rmse", train_rmse)
            mlflow.log_metric("train_r2", train_r2)
            mlflow.log_metric("validation_rmse", val_rmse)
            mlflow.log_metric("validation_r2", val_r2)
            mlflow.log_metric("test_rmse", test_rmse)
            mlflow.log_metric("test_r2", test_r2)
            
            # Calculate additional metrics
            overfitting_gap = train_r2 - test_r2
            mlflow.log_metric("overfitting_gap", overfitting_gap)
            mlflow.log_metric("test_accuracy_percent", test_r2 * 100)
            
            print(f"‚úÖ METRICS LOGGED:")
            print(f"   Train R¬≤: {train_r2:.4f}")
            print(f"   Validation R¬≤: {val_r2:.4f}")
            print(f"   Test R¬≤: {test_r2:.4f} ({test_r2*100:.2f}%)")
            
            # 10. FEATURE IMPORTANCE
            if hasattr(model.stages[-1], 'featureImportances'):
                print("\nüîü FEATURE IMPORTANCE...")
                
                importances = model.stages[-1].featureImportances.toArray()
                feature_importance = list(zip(features, importances))
                feature_importance.sort(key=lambda x: x[1], reverse=True)
                
                # Log feature importance
                for i, (feature, importance) in enumerate(feature_importance):
                    mlflow.log_metric(f"feature_importance_{feature}", importance)
                
                # Create feature importance dict for artifacts
                importance_dict = {feature: float(importance) for feature, importance in feature_importance}
                
                # Save as JSON artifact
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json.dump(importance_dict, f, indent=2)
                    mlflow.log_artifact(f.name, "feature_importance.json")
                
                print(f"‚úÖ Feature importance logged (top feature: {feature_importance[0][0]} = {feature_importance[0][1]:.4f})")
            
            # 11. MODEL LOGGING
            print("\n1Ô∏è‚É£1Ô∏è‚É£ MODEL LOGGING...")
            
            # Create sample for signature
            sample_input = test_df.select(features).limit(5).toPandas()
            sample_output = test_predictions.select("prediction").limit(5).toPandas()
            
            signature = infer_signature(sample_input, sample_output)
            
            # Log model with MLflow
            model_info = mlflow.spark.log_model(
                spark_model=model,
                artifact_path="model",
                signature=signature,
                input_example=sample_input.iloc[0:1],
                registered_model_name="Ultimate_Energy_Forecasting_Model"
            )
            
            mlflow.log_param("model_uri", model_info.model_uri)
            
            print(f"‚úÖ Model logged: {model_info.model_uri}")
            
            # 12. ADDITIONAL ARTIFACTS
            print("\n1Ô∏è‚É£2Ô∏è‚É£ ADDITIONAL ARTIFACTS...")
            
            # Log run summary
            overall_time = time.time() - overall_start
            mlflow.log_metric("total_pipeline_time_seconds", overall_time)
            mlflow.log_metric("total_pipeline_time_minutes", overall_time / 60)
            
            # Final summary
            summary = {
                "success": True,
                "model_uri": model_info.model_uri,
                "test_r2": test_r2,
                "test_accuracy_percent": test_r2 * 100,
                "total_records": record_count,
                "daily_records": daily_count,
                "pipeline_time_minutes": overall_time / 60,
                "run_id": run.info.run_id
            }
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(summary, f, indent=2)
                mlflow.log_artifact(f.name, "run_summary.json")
            
            print(f"\nüéâ ULTIMATE + MLFLOW SUCCESS!")
            print("="*80)
            print(f"üìä Test Accuracy: {test_r2*100:.2f}%")
            print(f"üÜî MLflow Run ID: {run.info.run_id}")
            print(f"üè∑Ô∏è  Model URI: {model_info.model_uri}")
            print(f"üìÅ Tracking URI: {tracking_uri}")
            print(f"‚è±Ô∏è  Total time: {overall_time/60:.1f} minutes")
            print(f"‚úÖ ENTERPRISE MODEL TRACKED & REGISTERED!")
            
            return summary
            
        except Exception as e:
            mlflow.log_param("error", str(e))
            print(f"‚ùå Error: {e}")
            raise
            
        finally:
            if 'spark' in locals():
                spark.stop()
                print("üßπ Spark session stopped")

if __name__ == "__main__":
    print("üöÄ ULTIMATE + MLFLOW ENTERPRISE PIPELINE")
    print("üìä Model Registry + Experiment Tracking + Artifact Storage")
    
    try:
        summary = ultimate_mlflow_pipeline()
        
        print(f"\nüéä ULTIMATE MLFLOW SUCCESS!")
        print(f"üÜî Run ID: {summary['run_id']}")
        print(f"üìä Test Accuracy: {summary['test_accuracy_percent']:.2f}%")
        print(f"üè∑Ô∏è  Model registered and tracked!")
        print(f"\n‚úÖ ENTERPRISE MLOPS MASTERY ACHIEVED! üèÜ")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
